server:
  port: 8080

spring:
  application:
    name: message-center
  
  # 数据源配置
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://${MYSQL_HOST:localhost}:${MYSQL_PORT:3306}/message_center?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true
    username: ${MYSQL_USERNAME:root}
    password: ${MYSQL_PASSWORD:root}
    druid:
      # 初始化时建立物理连接的个数
      initial-size: 5
      # 最小连接池数量
      min-idle: 5
      # 最大连接池数量
      max-active: 20
      # 获取连接时最大等待时间，单位毫秒
      max-wait: 60000
      # 连接有效性检测时间
      time-between-eviction-runs-millis: 60000
      # 连接在池中最小生存的时间
      min-evictable-idle-time-millis: 300000
      # 连接在池中最大生存的时间
      max-evictable-idle-time-millis: 900000
      # 检测连接是否有效的SQL
      validation-query: SELECT 1 FROM DUAL
      # 申请连接时执行validationQuery检测连接是否有效
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      # 配置监控统计拦截的filters
      filters: stat,wall
      # 是否缓存preparedStatement
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      
  # Kafka配置
  kafka:
    bootstrap-servers: ${KAFKA_SERVERS:localhost:9092}
    producer:
      # 消息发送失败重试次数
      retries: 3
      # 批量发送消息大小
      batch-size: 16384
      # 缓冲区内存大小
      buffer-memory: 33554432
      # 消息key序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 消息value序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 消息确认模式 all: 所有副本确认
      acks: all
      # 压缩类型
      compression-type: gzip
    consumer:
      # 消费者组
      group-id: message-center-consumer-group
      # 消息key反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 消息value反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 自动提交offset
      enable-auto-commit: false
      # 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      auto-offset-reset: latest
      # 每次poll最大消息数量
      max-poll-records: 100
    listener:
      # 消费者监听器并发数
      concurrency: 3
      # 手动提交模式
      ack-mode: manual_immediate
      
  # Redis配置
  redis:
    host: ${REDIS_HOST:localhost}
    port: ${REDIS_PORT:6379}
    password: ${REDIS_PASSWORD:}
    database: ${REDIS_DATABASE:0}
    timeout: 5000ms
    lettuce:
      pool:
        # 连接池最大连接数
        max-active: 20
        # 连接池最大阻塞等待时间
        max-wait: 60000ms
        # 连接池中的最大空闲连接
        max-idle: 10
        # 连接池中的最小空闲连接
        min-idle: 5
        
  # Elasticsearch配置
  elasticsearch:
    uris: ${ES_URIS:http://localhost:9200}
    username: ${ES_USERNAME:}
    password: ${ES_PASSWORD:}
    connection-timeout: 5s
    socket-timeout: 30s

# MyBatis-Plus配置
mybatis-plus:
  # mapper xml文件路径
  mapper-locations: classpath*:/mapper/**/*.xml
  # 实体类路径
  type-aliases-package: cn.gt.msg.entity
  configuration:
    # 开启驼峰命名转换
    map-underscore-to-camel-case: true
    # 日志实现
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
  global-config:
    db-config:
      # 主键类型
      id-type: auto
      # 逻辑删除配置
      logic-delete-field: deleted
      logic-delete-value: 1
      logic-not-delete-value: 0

# Apollo配置
apollo:
  meta: ${APOLLO_META:http://localhost:8080}
  bootstrap:
    enabled: ${APOLLO_ENABLED:true}
    namespaces: ${APOLLO_NAMESPACES:application,message-center}
  
# 消息中心自定义配置
message-center:
  # 重试配置
  retry:
    enabled: true
    # 重试间隔(分钟)
    intervals: 10,30
    # 最大重试次数
    max-count: 2
  # 推送配置
  push:
    # 默认超时时间(ms)
    default-timeout: 30000
    # 线程池配置
    thread-pool:
      core-size: 10
      max-size: 50
      queue-capacity: 200
      keep-alive-seconds: 60
  # ES日志配置
  elasticsearch:
    # 生产者日志索引
    producer-log-index: producer_message_log_index
    # 消费者日志索引
    consumer-log-index: consumer_push_log_index
    # 批量写入大小
    bulk-size: 100
    # 批量写入间隔(秒)
    bulk-interval: 5

# 日志配置
logging:
  level:
    root: INFO
    cn.gt.msg: DEBUG
    org.springframework.kafka: INFO
    org.apache.kafka: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n"
  file:
    name: logs/message-center.log
    max-size: 100MB
    max-history: 30
